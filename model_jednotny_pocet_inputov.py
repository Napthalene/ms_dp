# -*- coding: utf-8 -*-
"""Model_jednotny_pocet_inputov.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t43RyxsWyStFfmOBvjbFUVQmhXqXnCOW
"""

# from pydrive.auth import GoogleAuth
# from pydrive.drive import GoogleDrive
#
# gauth = GoogleAuth()
# gauth.LocalWebserverAuth()
# drive = GoogleDrive(gauth)

# Pripojenie k Google Drive-u
# from google.colab import drive
# drive.mount('/content/drive')

# Importy
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
# import pandas as pd
import time
import csv
import random
import os
import udf

# Pozriet co robi: from torch.utils.tensorboard import SummaryWriter

# Kontrola dostupnosti CUDA
if torch.cuda.is_available():
    device = torch.device("cuda:0")  # Pouzitie GPU
    print("Pouzitie GPU.")
else:
    device = torch.device("cpu")     # Pouzitie CPU ak CUDA nie je dostupna
    print("Pouzitie CPU.")

"""# **Pomocne Funkcie**"""

# Nacitavanie random input datasetov
def load_random_datasets(random_data_indexes, priecinok_cesta = f"{os.getcwd()}\\data"):
  dataset = np.genfromtxt(f"{priecinok_cesta}/FRF_databaza/FRF_{1}.csv", delimiter=',', skip_header=0)
  data_len = dataset[0,:].__len__()
  data_num = random_data_indexes.shape[1]
  datasets = np.zeros([data_num, 1, data_len*2])
  nacitavanie_string = "Nacitavanie inputov"
  # print(nacitavanie_string, end="")
  for index, random_data_index in enumerate(random_data_indexes[0, :]):
    if index % 10 == 0:
      if nacitavanie_string == "Nacitavanie inputov...":
        nacitavanie_string = "Nacitavanie inputov"
      else:
        nacitavanie_string += "."
    print(f"\r{index+1}/{data_num} {nacitavanie_string}", end="")
    dataset = np.genfromtxt(f"{priecinok_cesta}/FRF_databaza/FRF_{random_data_index}.csv", delimiter=',', skip_header=0)
    datasets[index, 0, :data_len] = dataset[0, :]
    datasets[index, 0, data_len:] = np.log(dataset[1, :])  # Logaritmovanie y hodnoty
  print(f"\r{index+1}/{data_num} Inputy nacitane")
  return datasets

# Nacitanie random verification datasetov
def load_random_verification_dataset(random_data_indexes, priecinok_cesta = f"{os.getcwd()}\\data"):
  dataset = np.genfromtxt(f"{priecinok_cesta}/VUT_&_pomerne_tlmenie/vut_pomerne_tlmenie_{1}.csv", delimiter=',', skip_header=0)
  data_len = dataset[0,:].__len__()
  data_num = random_data_indexes.shape[1]
  datasets = np.zeros([data_num, 2, data_len])
  nacitavanie_string = "Nacitavanie outputov"
  print(nacitavanie_string, end="")
  for index, random_data_index in enumerate(random_data_indexes[0, :]):
    if index % 10 == 0:
      if nacitavanie_string == "Nacitavanie outputov...":
        nacitavanie_string = "Nacitavanie outputov"
      else:
        nacitavanie_string += "."
    print(f"\r{index+1}/{data_num} {nacitavanie_string}", end="")
    dataset = np.genfromtxt(f"{priecinok_cesta}/VUT_&_pomerne_tlmenie/vut_pomerne_tlmenie_{random_data_index}.csv", delimiter=',', skip_header=0)
    datasets[index, 0, :] = dataset[0, :]
    datasets[index, 1, :] = dataset[1, :]
  print(f"\r{index+1}/{data_num} Overovacie outputy nacitane")
  return datasets

# Nacitanie dat na verifikaciu
def load_verification_datasets(data_index, priecinok_cesta = f"{os.getcwd()}\\data"):
  input_dataset = np.genfromtxt(f"{priecinok_cesta}/FRF_databaza/FRF_{data_index}.csv", delimiter=',', skip_header=0)
  data_len = input_dataset[0, :].__len__()
  verification_input = np.zeros([1, 1, data_len*2])
  verification_input[0, 0, :data_len] = input_dataset[0, :]
  verification_input[0, 0, data_len:] = np.log(input_dataset[1, :])
  output_dataset = np.genfromtxt(f"{priecinok_cesta}/VUT_&_pomerne_tlmenie/vut_pomerne_tlmenie_{data_index}.csv", delimiter=',', skip_header=0)
  data_len = output_dataset[0, :].__len__()
  verification_output = np.zeros([1, 2, data_len])
  verification_output[0, 0, :] = output_dataset[0, :]
  verification_output[0, 1, :] = output_dataset[1, :]
  return verification_input, verification_output

# Nacitanie input data do jedneho velkeho datasetu (zlogartimuje y hodnoty)
def load_all_input_data(start_data_num, end_data_num, priecinok_cesta = f"{os.getcwd()}\\data"):
  dataset = np.genfromtxt(f"{priecinok_cesta}/FRF_databaza/FRF_{1}.csv", delimiter=',', skip_header=0)
  data_len = dataset[0,:].__len__()
  data_num = end_data_num - start_data_num + 1
  datasets = np.zeros([data_num, 1, data_len*2])
  end_data_num += 1
  nacitavanie_string = "Nacitavanie inputov"
  print(nacitavanie_string, end="")
  for index, data_index in enumerate(range(start_data_num, end_data_num)):
    if index % 10 == 0:
      if nacitavanie_string == "Nacitavanie inputov...":
        nacitavanie_string = "Nacitavanie inputov"
      else:
        nacitavanie_string += "."
      print(f"\r{data_index-start_data_num+1}/{data_num} {nacitavanie_string}", end="")
    dataset = np.genfromtxt(f"{priecinok_cesta}/FRF_databaza/FRF_{data_index}.csv", delimiter=',', skip_header=0)
    datasets[index, 0, :data_len] = dataset[0, :]
    datasets[index, 0, data_len:] = np.log(dataset[1, :])  # Logaritmovanie y hodnoty
  return datasets

# Nacitanie verification output dat do jedneho datasetu
def load_verification_data(start_data_num, end_data_num, priecinok_cesta = f"{os.getcwd()}\\data"):
  dataset = np.genfromtxt(f"{priecinok_cesta}/VUT_&_pomerne_tlmenie/vut_pomerne_tlmenie_{1}.csv", delimiter=',', skip_header=0)
  data_len = dataset[0,:].__len__()
  data_num = end_data_num - start_data_num + 1
  datasets = np.zeros([data_num, 2, data_len])
  end_data_num += 1
  nacitavanie_string = "Nacitavanie outputov"
  print(nacitavanie_string, end="")
  for index, data_index in enumerate(range(start_data_num, end_data_num)):
    if index % 10 == 0:
      if nacitavanie_string == "Nacitavanie outputov...":
        nacitavanie_string = "Nacitavanie outputov"
      else:
        nacitavanie_string += "."
      print(f"\r{data_index-start_data_num+1}/{data_num} {nacitavanie_string}", end="")
    dataset = np.genfromtxt(f"{priecinok_cesta}/VUT_&_pomerne_tlmenie/vut_pomerne_tlmenie_{data_index}.csv", delimiter=',', skip_header=0)
    datasets[index, 0, :] = dataset[0, :]
    datasets[index, 1, :] = dataset[1, :]
  return datasets

# Testovanie load_all_input_data
start_data, end_data = (1, 1)
test_flag = False

if test_flag:
  # Nacitanie input dat
  temp_timer = time.time()
  datasets_input = load_all_input_data(start_data, end_data)
  time_left = round(time.time() - temp_timer)
  print(f"Doba nacitania input dat (od {start_data} do {end_data}): {int(time_left // 60)} min {int(time_left % 60)} s")

  # Nacitanie output dat
  temp_timer = time.time()
  datasets_output = load_verification_data(start_data, end_data)
  time_left = round(time.time() - temp_timer)
  print(f"Doba nacitania output dat (od {start_data} do {end_data}): {int(time_left // 60)} min {int(time_left % 60)} s")

  # Zobrazenie grafu spravnosti
  data_len = int(datasets_input[0, 0, :].__len__() / 2)
  plt.plot(datasets_input[0, 0, :data_len], datasets_input[0, 0, data_len:])
  plt.scatter(datasets_output[0, 0, :], [0, 0])
  plt.grid()

# Vytvaranie nahodnych batch-ov datasetov
def create_random_choice(num_of_data_in_batch, start_dataset, end_dataset):
    num_of_datasets = end_dataset - start_dataset + 1
    mylist = np.linspace(start_dataset, end_dataset, num_of_datasets, dtype=int).tolist()
    i = 0
    num_of_batches = num_of_datasets//num_of_data_in_batch
    random_datasets = np.zeros([num_of_batches, 1, num_of_data_in_batch], dtype=int)
    dataset_index = 0
    while True:
        temp = random.choice(mylist)
        mylist.remove(temp)
        # print(f"{i} temp:", temp)
        random_datasets[dataset_index, 0, i] = temp
        i += 1
        if i >= num_of_data_in_batch:
            dataset_index += 1
            i = 0
        if dataset_index >= num_of_batches:
            break
    return random_datasets

def verifikacia_modelu(batch_num):
  print("+"*50)
  print(f"Verifikacia modelu na datach c.{batch_num}") #batches[0, 0 ,0]
  print("+"*50)
  verification_input, verification_output = load_verification_datasets(batch_num)
  ver_inp = model(torch.from_numpy(verification_input).type(torch.float32).to(device))
  ver_inp = ver_inp.cpu()
  prva_ver = 100 - abs((ver_inp.detach()[0, 0, 0].tolist() - verification_output[0, 0, 0]) / verification_output[0, 0, 0]) * 100
  druha_ver = 100 - abs((ver_inp.detach()[0, 0, 1].tolist() - verification_output[0, 0, 1]) / verification_output[0, 0, 1]) * 100
  accuracy = round((prva_ver + druha_ver) / 2, 2)
  print(f"Odhadnute hodnoty:    {np.round(ver_inp.detach()[0, 0, :].tolist(), 2)}")
  print(f"Simulovane hodnoty:   {np.round(verification_output[0, 0, :].tolist(), 2)}")
  print(f"Presnost (Accuracy):  {accuracy}% ({round(prva_ver)}%, {round(druha_ver)}%)\n")

# Fukcia na vypocet casu
def calc_time(time_left):
  hod = f"0{int(time_left // (60*60))}" if len(str(int(time_left // (60*60)))) < 2 else int(time_left // (60*60))
  min = f"0{int((time_left % (60*60)) // 60)}" if len(str(int(time_left % (60*60) // 60))) < 2 else int((time_left % (60*60)) // 60)
  sec = f"0{int(time_left % (60*60) % 60)}" if len(str(int(time_left % (60*60) % 60))) < 2 else int(time_left % (60*60) % 60)
  return hod, min, sec

"""# **Definovanie Modelu**
Model ma jednotný počet vstupných parametrov, to znamená, že input layer má 2500 neuronov.
"""

# Definicia Modelu/Neuralnej Siete
class VUF_odhadovac(nn.Module):
    def __init__(self, n_input_layers, n_output_layers):
        super(VUF_odhadovac, self).__init__()
        # Zadefinovanie vrstiev
        self.sequential_layers = nn.Sequential(
            nn.Linear(n_input_layers, 2500),
            nn.ReLU(),
            nn.Linear(2500, 1250),
            nn.ReLU(),
            nn.Linear(1250, 625),
            nn.ReLU()
        )
        self.output_layer = nn.Linear(625, n_output_layers) # Output vrstva

    def forward(self, x):
        # Definicia forward propagation
        x = self.sequential_layers(x)
        x = self.output_layer(x)  # Posledna vrstva ma linernu aktivacnu funkciu
        return x

"""# **Nacitanie vytrenovaneho modelu**

"""

# Nacitanie vytrenovaneho modelu
# Vyber poctu neuronov input vrstvy
priecinok_cesta = f"{os.getcwd()}"
n_input_layers = np.genfromtxt(f"{priecinok_cesta}\\data/FRF_databaza/FRF_{1}.csv", delimiter=',', skip_header=0).shape[1]*2
n_output_layers = 2

# Inicializacia Modelu
model = VUF_odhadovac(n_input_layers, n_output_layers).to(device)
state_dict = torch.load(f"{priecinok_cesta}\\models/model_200_2k3k.pth", map_location=torch.device(device))
model.load_state_dict(state_dict)
criterion = nn.MSELoss()  # Mean Squared Error Loss
optimizer = torch.optim.Adam(model.parameters())
optimizer.zero_grad()

# model.eval()  # Nastavenie modelu do režimu inferencie
model.train()

"""# **Trenovanie**

---


**Trenovanie na viacerych datasetoch**

---



Hlavna funkcia trenovania na viacerych datasetoch s moznostou zmenit trenovacie data
"""

# Definovanie nacitania dat
# start_data  -> cislo PRVYCH dat intervalu na ktorom sa bude trenovat
# end_data    -> cislo POSLEDNYCH dat intervalu na ktorom sa bude trenovat
# num_of_data_in_batch -> pocet nahodnych datasetov vchadzjucich do trenovacieho procesu
start_data, end_data = (1, 1000)
num_of_data_in_batch = 500

# Pocet epoch
epochs = 200

print(f"Nacitane - pocet trenovacich datasetov: {end_data}\n\
         - pocet datasetov v batch-i:   {num_of_data_in_batch}\n\
         - pocet epoch:                 {epochs}"\
      if (end_data % num_of_data_in_batch == 0) or end_data < num_of_data_in_batch else\
      "Zadane hodnoty nie su spravne")

# Vyber poctu neuronov input vrstvy
priecinok_cesta = f"{os.getcwd()}\\data"
n_input_layers = np.genfromtxt(f"{priecinok_cesta}/FRF_databaza/FRF_{1}.csv", delimiter=',', skip_header=0).shape[1]*2
n_output_layers = 2

# Inicializacia Modelu
model = VUF_odhadovac(n_input_layers, n_output_layers).to(device)

# Loss & Optimizer
criterion = nn.MSELoss()  # Mean Squared Error Loss
torch.manual_seed(111)  # Zapnut ak chcem reprodukovatelne data
optimizer = torch.optim.Adam(model.parameters())  # Adam Optimizer; tu sa da doplnit este learning rate
optimizer.zero_grad()

# INFOAppData\Roaming\JetBrains
print(f"Pocet neuronov vo vstupnej vrstve:  {n_input_layers}")
print(f"Pocet neuronov vo vystupnej vrstve: {n_output_layers}")
print("\nInicializacia modelu prebehla uspesne")

# Trenovacia funkcia s random datami
model_specification = "_200_2k3k"
priecinok_cesta = f"{os.getcwd()}\\models"
times = list()
for epoch in range(epochs):
  state_dict = model.state_dict()
  torch.save(state_dict, f"{priecinok_cesta}/model{model_specification}.pth")
  print("Model bol ulozeny!")
  # Nahodne rozhadzanie dat do batch-ov
  batches = create_random_choice(num_of_data_in_batch, start_data, end_data)
  if epoch >= 1:
    verifikacia_modelu(batches[0, 0, 0])
  # Nacitavanie jednotlivych batch-ov
  for batch_index, batch in enumerate(batches):
    print("-"*50)
    print(f"Epocha: {epoch+1}/{epochs}, Batch: {batch_index+1}/{batches.shape[0]}")
    print("-"*50)
    timer_load = time.time()
    input_datasets = load_random_datasets(batch)
    verification_datasets = load_random_verification_dataset(batch)
    timer_load = time.time() - timer_load
    # Trenovanie nacitaneho batchu
    print("Trenovanie:")
    timer_batch_train = time.time()
    for index, dataset in enumerate(input_datasets):
      timer_start = time.time()
      optimizer.zero_grad() # Vynulovanie gradientu
      input_data = torch.from_numpy(dataset[0, :]).type(torch.float32).to(device)
      verification_data = torch.from_numpy(verification_datasets[index, 0, :]).type(torch.float32).to(device)
      outputs = model(input_data)
      loss = criterion(outputs, verification_data)
      loss.backward()
      optimizer.step()
      if times.__len__() > 100:
        times.pop(0)
      times.append(time.time() - timer_start)
      if epoch+1 == epochs:
        time_left = round((epochs*(end_data-start_data) - (epoch*(end_data-start_data) + batch_index*num_of_data_in_batch + index)) * np.mean(times))
      else:
        time_left = round(((epochs*(end_data-start_data) - (epoch*(end_data-start_data) + batch_index*num_of_data_in_batch + index)) * np.mean(times)) + timer_load*(epochs-(epoch+1)))
      hod, min, sec = calc_time(time_left)
      print(f"\rOdhadovany cas vypoctu: {hod}h {min}m {sec}s", end="")
    print("\n")
verifikacia_modelu(batches[0, 0, 0])
state_dict = model.state_dict()
torch.save(state_dict, f"{priecinok_cesta}/model{model_specification}.pth")
print("Model bol ulozeny!")
print("Trenovanie dokoncene!")

# Testovanie
start_test, end_test = (990, 1000)
for i in range(start_test, end_test+1):
  temp_1, temp_2 = load_verification_datasets(i)
  odhadnute_vuf = model(torch.from_numpy(temp_1).type(torch.float32)).detach()
  index = int(temp_1.shape[2]/2)
  print(f"Odhadnute VUF:  {odhadnute_vuf[0, 0, :].tolist()}")
  print(f"Simulovane VUF: {temp_2[0, 0, :]}")
  plt.title(f"Dataset {i}")
  plt.plot(temp_1[0, 0, :index], temp_1[0, 0, index:])
  plt.scatter(temp_2[0, 0, :], [0, 0], label="Simulovane VUF", color="black")
  plt.scatter(odhadnute_vuf, [0, 0], label="Odhadnute VUF", color="red")
  plt.legend(), plt.grid(), plt.show()

"""# **Trenovanie vsetkych dat naraz**"""

# Nacitanie datasetov
timer_nacitanie_dat = time.time()
input_datasets = load_all_input_data(start_data, end_data)
verification_datasets = load_verification_data(start_data, end_data)
timer_nacitanie_dat = round(time.time() - timer_nacitanie_dat)
number_of_datasets = end_data - start_data + 1

# Info
print(f"\rNacitanie datasetov prebehlo uspesne, trvanie: {int(timer_nacitanie_dat // 60)} min {int(timer_nacitanie_dat % 60)} s")
print(f"Trenovanie na datach {start_data} az {end_data}")

# Trenovanie
times = list()
for epoch in range(epochs):
  optimizer.zero_grad()
  for index, input_dataset in enumerate(input_datasets):
    timer_start = time.time()
    # Nacitanie dat
    input_data = torch.from_numpy(input_dataset[0, :]).type(torch.float32)
    verification_data =  torch.from_numpy(verification_datasets[index, 0, :]).type(torch.float32)
    outputs = model(input_data)
    loss = criterion(outputs, verification_data)
    loss.backward()
    optimizer.step()
    if times.__len__() > 100:
      times.pop(0)
    times.append(time.time() - timer_start)
    time_left = round((((epochs*(number_of_datasets) - (epoch*(number_of_datasets) + index))) * np.mean(times)), 2)
    hod = f"0{int(time_left // (60*60))}" if len(str(int(time_left // (60*60)))) < 2 else int(time_left // (60*60))
    min = f"0{int((time_left % (60*60)) // 60)}" if len(str(int(time_left % (60*60) // 60))) < 2 else int((time_left % (60*60)) // 60)
    sec = f"0{int(time_left % (60*60) % 60)}" if len(str(int(time_left % (60*60) % 60))) < 2 else int(time_left % (60*60) % 60)
    print(f"\rEpocha: {epoch+1}/{epochs}; Odhadovany cas vypoctu: {hod}h {min}m {sec}s", end="")
print("\nTrenovanie uspesne ukoncene")

# Testovanie

TbWriter = udf.TbWriter
logdir = f"{os.getcwd()}\\logs"
tb_writer = TbWriter(logdir)
tb_writer.write(it=it, reward_legal=reward_legal,
                reward_group=reward_group, reward=reward,
                score_legal_rm=score_legal_rm,
                score_group_rm=score_group_rm,
                score_rm=score_rm, img=img, img_rm=img_rm)